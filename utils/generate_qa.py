import json
import random
import os
import re

def get_hash2name():
    with open("processeddata/data3.json") as f:
        data = json.load(f)
    all_models = {}
    for key, value in data.items():
        if 'Model hash' in value and 'Model' in value:
            if value['Model hash'] not in all_models:
                all_models[value['Model hash']] = [value['Model']]
            else:
                all_models[value['Model hash']].append(value['Model'])
    hash2name = {}
    num_per_model = {}
    for key, value in all_models.items():
        hash2name[key] = set(value)
        num_per_model[key] = len(value)
    return hash2name, num_per_model

hash2name = {}
with open('processeddata/trainhash.txt', 'r') as f:
    for line in f:
        parts = line.split()
        key = parts[0]
        values = parts[1]
        hash2name[key] = values

def coco_sample(file_path, train_number=10000, test_number=1000):
    dataset = json.load(open(file_path, "r"))
    return_files = []
    for file in dataset:
        Q1 = "Is this image generated by AI?"
        A1 = "This is a real image."
        return_files.append({"input": Q1, 'output': A1, "image": file["image"], "image_id": file["image_id"]})
    random.shuffle(return_files)
    train_files = return_files[:train_number]
    test_files = return_files[train_number:train_number+test_number]
    return train_files, test_files

def dddb_real_sample(root_dir, train_number=10000, test_number=1000):
    files = os.listdir(root_dir)
    return_files = []
    for i, file_name in enumerate(files):
        image = 'real/' + file_name
        Q1 = "Is this image generated by AI?"
        A1 = "This is a real image."
        return_files.append({"input": Q1, 'output': A1, "image": image, "image_id": image})
    # random.shuffle(return_files)
    # train_files = return_files[:train_number]
    # test_files = return_files[train_number:train_number+test_number]
    return return_files #train_files, test_files

def dalle_sample(file_path, train_number=10000, test_number=2000):
    prefix = 'dalle'
    dataset = json.load(open(file_path, "r"))
    keys = list(dataset.keys())
    # random.shuffle(keys)
    # selected_train = keys[:train_number]
    # selected_test = keys[train_number:train_number+test_number]
    train_files_det, train_files_qa=[],[]
    # for key in selected_train:
    for key in keys:
        value = dataset[key]
        model_name = 'DALLE'
        text = value['Prompt']
        if text is not None:
            text = re.sub(r'\n+', ' ', text)
            text = re.sub(r'\n', '', text)
            if len(text)>10:
                image_id = key.split('\\')[-1]
                image_path = '{}/{}'.format(prefix, image_id)
                Q1 = "Is this image generated by AI?"
                A1 = "This is an ai generated image by {}.".format(model_name)
                train_files_det.append({"input": Q1, 'output': A1, "image": image_path, "image_id": image_id})
                Q2 = "Give me prompts using DALLE to generate this image."
                A2 = "You can use this prompt: {}".format(text)
                train_files_qa.append({"input": Q2, 'output': A2, "image": image_path, "image_id": image_id})

    return train_files_det, train_files_qa #, test_files_det, test_files_qa



def mj_sample(file_path="processeddata/mj_up.json", train_number=10000, test_number=1000):
    data = json.load(open(file_path, "r"))
    root_dir = "rawimages/mj"
    files = os.listdir(root_dir)
    train_files_det, train_files_qa=[],[]
    # for i, file_name in enumerate(selected_train):
    for i, file_name in enumerate(files):
        key = 'mj/' + file_name
        values = data[key]
        input_string = values["Prompt"]
        cleaned_string = re.sub(r'<[^>]+>', '', input_string)
        cleaned_string = re.sub(r'\s*--[^\s\d]+(?:\s+\d+:\d+)*', '', cleaned_string)
        cleaned_string = re.sub(r'\s+', ' ', cleaned_string)
        cleaned_string = cleaned_string.strip()
        if cleaned_string is not None and len(cleaned_string)>10:
            model_name = "Midjourney"
            image_path = key
            image_id = file_name
            text = cleaned_string
            Q1 = "Is this image generated by AI?"
            A1 = "This is an ai generated image by {}.".format(model_name)
            train_files_det.append({"input": Q1, 'output': A1, "image": image_path, "image_id": image_id})
            Q2 = "Give me prompts using Midjourney to generate this image."
            A2 = "You can use this prompt: {}".format(text)
            train_files_qa.append({"input": Q2, 'output': A2, "image": image_path, "image_id": image_id})

    return train_files_det, train_files_qa #, test_files_det, test_files_qa


def sd_sample(file_path, train_number=10000, test_number=1000):
    prefix = 'sd'
    dataset = json.load(open(file_path, "r"))
    keys = list(dataset.keys())
    train_files_det, train_files_qa=[],[]
    # for key in selected_train:
    for key in keys:
        value = dataset[key]
        model_name = 'StableDiffusion'
        text = value['p']
        if text is not None and len(text)>10:
            image_id = key
            image_path = '{}/{}'.format(prefix, image_id)
            Q1 = "Is this image generated by AI?"
            A1 = "This is an ai generated image by {}.".format(model_name)
            train_files_det.append({"input": Q1, 'output': A1, "image": image_path, "image_id": image_id})
            Q2 = "Give me prompts using StableDiffusion to generate this image."
            A2 = "I suggest using {} and this prompt: {}".format(model_name, text)
            train_files_qa.append({"input": Q2, 'output': A2, "image": image_path, "image_id": image_id})
    return train_files_det, train_files_qa #, test_files_det, test_files_qa


def sdft_sample(data_path, hash2name):
    with open(data_path) as f:
        data = json.load(f)
    prefix = 'sdft'
    keys = list(data.keys())
    train_files_det, train_files_qa=[],[]
    for key in keys:
        value = data[key]
        if 'Prompt' in value and value['Prompt'] is not None:
            if 'Model' in value and value['Model'] == 'NovelAI':
                model_name = 'NovelAI'
                text = value['Prompt']
                clean_text = re.sub(r'[\[\](\n<>\.{} ]+|lora:.+?(?=\s*,|\s*$)+|\d+\)+|\)+|:\d', '', text).strip()
                clean_text = re.sub(r',+', ',', clean_text).strip().lstrip(", ")
                image_id = key.split('\\')[-1]
                image_path = '{}/{}'.format(prefix, image_id)
                if len(clean_text)>10:
                    Q1 = "Is this image generated by AI?"
                    A1 = "This is an ai generated image by StableDiffusion probably {}.".format(model_name)
                    train_files_det.append({"input": Q1, 'output': A1, "image": image_path, "image_id": image_id})
                    Q2 = "Give me prompts using StableDiffusion to generate this image."
                    A2 = "I suggest using {} and this prompt: {}".format(model_name, clean_text)
                    train_files_qa.append({"input": Q2, 'output': A2, "image": image_path, "image_id": image_id})
            elif 'Model hash' in value:
                if value['Model hash'] in hash2name:
                    model_name = hash2name[value['Model hash']]
                    text = value['Prompt']
                    clean_text = re.sub(r'[\[\](\n<>\.{} ]+|lora:.+?(?=\s*,|\s*$)+|\d+\)+|\)+|:\d', '', text).strip()
                    clean_text = re.sub(r',+', ',', clean_text).strip().lstrip(", ")
                    image_id = key.split('\\')[-1]
                    image_path = '{}/{}'.format(prefix, image_id)
                    if len(clean_text) > 10:
                        Q1 = "Is this image generated by AI?"
                        A1 = "This is an ai generated image by StableDiffusion probably {}.".format(model_name)
                        train_files_det.append({"input": Q1, 'output': A1, "image": image_path, "image_id": image_id})
                        Q2 = "Give me prompts using StableDiffusion to generate this image."
                        A2 = "I suggest using {} and this prompt: {}".format(model_name, clean_text)
                        train_files_qa.append({"input": Q2, 'output': A2, "image": image_path, "image_id": image_id})

    return train_files_det, train_files_qa #, test_files_det, test_files_qa

real_train_det = dddb_real_sample('rawimages/real')
sd_train_det, sd_train_qa = sd_sample('processeddata/sd.json')
sdft_train_det, sdft_train_qa = sdft_sample("processeddata/data3.json", hash2name)
dalle_train_det, dalle_train_qa  = dalle_sample('processeddata/dalle.json')
mj_train_det, mj_train_qa = mj_sample("processeddata/mj_up.json")

# len(real_train_det)
# 62154
# len(sdft_train_det)
# 49249
# len(sd_train_qa)
# 14862
# len(dalle_train_qa)
# 30961
# len(mj_train_qa)
# 20867
# length = 10000


train = real_train_det + sd_train_det + sdft_train_det \
        + dalle_train_det + mj_train_det \
        + sd_train_qa+sdft_train_qa+dalle_train_qa+mj_train_qa

random.shuffle(train)

with open("all_train.json", "w") as f:
    json.dump(train, f)


train = real_train_det[:10000] + sd_train_det[:2500] + sdft_train_det[:2500] \
        + dalle_train_det[:2500] + mj_train_det[:2500] \
        + sd_train_qa+sdft_train_qa[:40000]+dalle_train_qa[:2500]+mj_train_qa[:2500]
random.shuffle(train)

with open("balance_train.json", "w") as f:
    json.dump(train, f)


with open("sdft_test.json", "w") as f:
    json.dump(sdft_train_qa[40000:], f)



# create imagenet format, identification
def sdft_helper_testset(sdft_train_qa, numpermodel=100):
    new_list = []
    nummodle = {}
    for ii in sdft_train_qa:
        modelname = ii['output'].split()[3]
        print(modelname)
        if modelname not in nummodle:
            nummodle[modelname] = 1
        else:
            nummodle[modelname] += 1
        if nummodle[modelname]<=numpermodel:
            new_list.append(ii)
    return new_list

new_list =sdft_helper_testset(sdft_train_qa[40000:], numpermodel=100)



with open("sdft_test_100.json", "w") as f:
    json.dump(new_list, f)






